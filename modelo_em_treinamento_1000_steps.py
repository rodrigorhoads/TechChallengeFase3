# -*- coding: utf-8 -*-
"""modelo em treinamento 1000 steps.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16-5scSSE5dUUtkoeaOUBzX0BM5nF06Z8
"""

from google.colab import  drive
drive.mount('/content/drive', force_remount=True)

!pip install 'unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git'
!pip install --no-deps xformers 'trl<0.9.0' peft accelerate bitsandbytes
!pip install transformers dataset

from unsloth import FastLanguageModel, is_bfloat16_supported

import torch
import json
from datasets import Dataset
from trl import SFTTrainer
from transformers import TrainingArguments

OUTPUT_PATH_DATASET ='/content/drive/MyDrive/fase3/dataset/trnPrepared.json'

#import json

#processed_data =[]
#with open('/content/drive/MyDrive/fase3/trn.json','r', encoding='utf-8') as file:
#       for linha in file:
#        try:
#          linhaJson = json.loads(linha.strip())
#        except json.JSONDecodeError:
#          continue

#        if linhaJson["content"] != '' and linhaJson["content"] is not None and linhaJson["title"]!= '' and linhaJson["title"] is not None:
#          title = linhaJson["title"]
#          content = linhaJson["content"]
#          processed_data.append({"title":title,"content":content})

#with open(OUTPUT_PATH_DATASET,'w', encoding='utf-8') as file:
#  json.dump(processed_data,file,ensure_ascii=False,indent=1)

import pandas as pd

df = pd.read_json('/content/drive/MyDrive/fase3/dataset/trnPrepared.json')

df.drop_duplicates(inplace=True)

max_seq_lenth = 2048
dtype = None
load_in_4bit = True
fourbit_models = [
    "unsloth/mistral-7b-v0.3-bnb-4bit",
    "unsloth/mistral-7b-instruct-v0.3-bnb-4bit",
    "unsloth/llama-3-8b-bnb-4bit",
    "unsloth/llama-3-8b-Instruct-bnb-4bit",
    "unsloth/llama-3-70b-bnb-4bit",
    "unsloth/Phi-3-mini-4k-instruct",
    "unsloth/Phi-3-medium-4k-instruct",
    "unsloth/mistral-7b-bnb-4bit",
    "unsloth/gemma-7b-bnb-4bit",
]

model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = 'unsloth/llama-3-8b-bnb-4bit',
    max_seq_length = max_seq_lenth,
    dtype = dtype,
    load_in_4bit = load_in_4bit
)

if tokenizer.pad_token is None or tokenizer.pad_token == tokenizer.eos_token:
    tokenizer.pad_token = tokenizer.unk_token # Use o token UNK como pad

from unsloth.chat_templates import get_chat_template
tokenizer = get_chat_template(tokenizer, chat_template="llama-3")

model = FastLanguageModel.get_peft_model(
    model,
    target_modules = ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj',],
    lora_alpha = 16,
    lora_dropout = 0,
    bias = 'none',

    use_gradient_checkpointing = 'unsloth',
    random_state = 3407,
    use_rslora = False,
    loftq_config = None
)

qtd_linhas = 100000

dir_checkpoint = "/content/drive/MyDrive/fase3/checkpoint"

def formatting_prompts_func(example):
    texts = []
    for title, content in zip(example['title'], example['content']):
        messages = [
            {"role": "user", "content": f"Return the exact content associated with the title: {title}"},
            {"role": "assistant", "content": content},
        ]

        text = tokenizer.apply_chat_template(
            messages,
            tokenize = False,
            add_generation_prompt = False
        )
        texts.append(text)
    return {"text": texts}


dataset = Dataset.from_pandas(df.head(qtd_linhas))
dataset = dataset.map(formatting_prompts_func, batched = True,)

trainer = SFTTrainer(
    model = model,
    tokenizer = tokenizer,
    train_dataset = dataset,
    dataset_text_field = "text",
    max_seq_length = max_seq_lenth,
    dataset_num_proc = 2,
    packing = False,
    args = TrainingArguments(
        per_device_train_batch_size = 2,
        gradient_accumulation_steps = 4,
        warmup_steps = 5,
        max_steps = 5600,
        learning_rate = 2e-4,
        fp16 = not is_bfloat16_supported(),
        bf16 = is_bfloat16_supported(),
        logging_steps = 1,
        optim = "adamw_8bit",
        weight_decay = 0.01,
        lr_scheduler_type = "linear",
        seed = 3407,
        save_strategy = "steps",
        save_steps = 500,
        save_total_limit = 3,
        output_dir = dir_checkpoint,
        report_to = "none",
    ),
)

FastLanguageModel.for_inference(model)

novo_titulo_para_teste = "Autumn Story Brambly Hedge"
messages = [
    {"role": "user", "content": f"Return the exact content associated with the title: {novo_titulo_para_teste}"},
]

prompt_string = tokenizer.apply_chat_template(
    messages,
    tokenize = False,
    add_generation_prompt=True
)

inputs = tokenizer(
    prompt_string,
    return_tensors="pt",
    padding=True # Garante que attention_mask é gerado
)

inputs = {k: v.to("cuda") for k, v in inputs.items()}

from transformers import TextStreamer
text_streamer = TextStreamer(tokenizer)
_ = model.generate(
    **inputs,
    streamer = text_streamer,
    max_new_tokens = 256,
    use_cache=True,
    do_sample=False,
    repetition_penalty=1.1,
    eos_token_id=tokenizer.eos_token_id
    )

#trainer_stats = trainer.train()
trainer_stats=trainer.train(resume_from_checkpoint=True)

FastLanguageModel.for_inference(model)

novo_titulo_para_teste = "Autumn Story Brambly Hedge"
messages = [
    {"role": "user", "content": f"Return the exact content associated with the title: {novo_titulo_para_teste}"},
]

prompt_string = tokenizer.apply_chat_template(
    messages,
    tokenize = False,
    add_generation_prompt=True
)

inputs = tokenizer(
    prompt_string,
    return_tensors="pt",
    padding=True
)

inputs = {k: v.to("cuda") for k, v in inputs.items()}

from transformers import TextStreamer
text_streamer = TextStreamer(tokenizer)
_ = model.generate(
    **inputs,
    streamer = text_streamer,
    max_new_tokens = 256,
    use_cache=True,
    do_sample=False,
    repetition_penalty=1.1,
    eos_token_id=tokenizer.eos_token_id
    )

model.save_pretrained("/content/drive/MyDrive/RecursosPostech/modelfinal") # Local saving
tokenizer.save_pretrained("/content/drive/MyDrive/RecursosPostech/modelfinal")

if True:
    from unsloth import FastLanguageModel
    model, tokenizer = FastLanguageModel.from_pretrained(
        model_name = "/content/drive/MyDrive/RecursosPostech/modelfinal", # YOUR MODEL YOU USED FOR TRAINING
        max_seq_length = max_seq_lenth,
        dtype = dtype,
        load_in_4bit = load_in_4bit,
    )
    FastLanguageModel.for_inference(model)

novo_titulo_para_teste = "Mog's Kittens"
messages = [
    {"role": "user", "content": f"Return the exact content associated with the title: {novo_titulo_para_teste}"},
]

prompt_string = tokenizer.apply_chat_template(
    messages,
    tokenize = False,
    add_generation_prompt=True
)

inputs = tokenizer(
    prompt_string,
    return_tensors="pt",
    padding=True # Garante que attention_mask é gerado
)

inputs = {k: v.to("cuda") for k, v in inputs.items()}

from transformers import TextStreamer
text_streamer = TextStreamer(tokenizer)
_ = model.generate(
    **inputs,
    streamer = text_streamer,
    max_new_tokens = 256,
    use_cache=True,
    do_sample=False,
    repetition_penalty=1.1,
    eos_token_id=tokenizer.eos_token_id
    )